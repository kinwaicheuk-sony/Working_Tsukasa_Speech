INTROTXT = """# 
Repo -> [Hugging Face - ğŸ¤—](https://huggingface.co/Respair/Tsukasa_Speech/edit/main/app_tsuka.py)
This space uses Tsukasa (24khz).
**Check the Read me tabs down below.** <br>
Enjoy!
"""
import gradio as gr
import random
import importable
import torch
import os
from Utils.phonemize.mixed_phon import smart_phonemize
import numpy as np
import pickle
import re

def is_japanese(text):
    if not text:  # Handle empty string
        return False
        
    # Define ranges for Japanese characters
    japanese_ranges = [
        (0x3040, 0x309F),  # Hiragana
        (0x30A0, 0x30FF),  # Katakana
        (0x4E00, 0x9FFF),  # Kanji
        (0x3000, 0x303F),  # Japanese punctuation and symbols
        (0xFF00, 0xFFEF),  # Full-width characters
    ]
    
    # Define range for Latin alphabets
    latin_alphabet_ranges = [
        (0x0041, 0x005A),  # Uppercase Latin
        (0x0061, 0x007A),  # Lowercase Latin
    ]
    
    # Define symbols to skip
    symbols_to_skip = {'\'', '*', '!', '?', ',', '.', ':', ';', '-', '_', '(', ')', '[', ']', '{', '}', '"'}
    
    for char in text:
        if char.isspace() or char in symbols_to_skip:  # Skip spaces and specified symbols
            continue
            
        char_code = ord(char)
        
        # Check if the character is a Latin alphabet
        is_latin_char = False
        for start, end in latin_alphabet_ranges:
            if start <= char_code <= end:
                is_latin_char = True
                break
                
        if is_latin_char:
            return False  # Return False if a Latin alphabet character is found
            
        # Check if the character is a Japanese character
        is_japanese_char = False
        for start, end in japanese_ranges:
            if start <= char_code <= end:
                is_japanese_char = True
                break
                
        if not is_japanese_char:
            return False
            
    return True

  
  
voices = {}
example_texts = {}
prompts = []
inputs = []


theme = gr.themes.Base(
    font=[gr.themes.GoogleFont('Libre Franklin'), gr.themes.GoogleFont('Public Sans'), 'system-ui', 'sans-serif'],
)

from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule

voicelist = [v for v in os.listdir("reference_sample_wavs")]



for v in voicelist:
    voices[v] = f'reference_sample_wavs/{v}'
    

with open(f'Inference/random_texts.txt', 'r') as r:
    random_texts = [line.strip() for line in r]

    example_texts = {f"{text[:30]}...": text for text in random_texts}
    
def update_text_input(preview):

    return example_texts[preview]

def get_random_text():
    return random.choice(random_texts)



with open('Inference/prompt.txt', 'r') as p:
    prompts = [line.strip() for line in p]
    
with open('Inference/input_for_prompt.txt', 'r') as i:
    inputs = [line.strip() for line in i]


last_idx = None

def get_random_prompt_pair():
    global last_idx
    max_idx = min(len(prompts), len(inputs)) - 1
    

    random_idx = random.randint(0, max_idx)
    while random_idx == last_idx:
        random_idx = random.randint(0, max_idx)
    
    last_idx = random_idx
    return inputs[random_idx], prompts[random_idx]

def Synthesize_Audio(text, voice, voice2, vcsteps, embscale, alpha, beta, ros, progress=gr.Progress()):

    
    text = smart_phonemize(text)
    
    
    if voice2 is not None:
        voice2 = {"path": voice2, "meta": {"_type": "gradio.FileData"}} 
        print(voice2)
        voice_style = importable.compute_style_through_clip(voice2['path'])
        
    else:  
        voice_style = importable.compute_style_through_clip(voices[voice])
    
    wav = importable.inference(
        text, 
        voice_style,
        alpha=alpha, 
        beta=beta, 
        diffusion_steps=vcsteps, 
        embedding_scale=embscale, 
        rate_of_speech=ros
    )

    return (24000, wav)

    
def LongformSynth_Text(text, s_prev=None, Kotodama=None, alpha=.0, beta=0, t=.8, diffusion_steps=5, embedding_scale=1, rate_of_speech=1.):

    japanese = text

    # raw_jpn = japanese[japanese.find(":") + 2:]
    # speaker = japanese[:japanese.find(":") + 2]


    if ":" in japanese[:10]:
        raw_jpn = japanese[japanese.find(":") + 2:]
        speaker = japanese[:japanese.find(":") + 2]
    else:
        raw_jpn = japanese
        speaker = ""
        
    sentences = importable.sent_tokenizer.tokenize(raw_jpn)
    sentences = importable.merging_sentences(sentences)
    
    
    # if is_japanese(raw_jpn):
    #   kotodama_prompt = kotodama_prompt

      
    # else:
    #   kotodama_prompt = speaker + importable.p2g(smart_phonemize(raw_jpn))
    #   print('kimia activated! the converted text is: ', kotodama_prompt)
      


    silence = 24000 * 0.5 # 500 ms of silence between outputs for a more natural transition
    # sentences = sent_tokenize(text)
    print(sentences)
    wavs = []
    s_prev = None
    for text in sentences:
        
        text_input = smart_phonemize(text)
        print('phonemes -> ', text_input)
        
        if is_japanese(text):
          kotodama_prompt = text

          
        else:
          kotodama_prompt = importable.p2g(smart_phonemize(text))
          kotodama_prompt = re.sub(r'\s+', ' ', kotodama_prompt).strip()
          print('kimia activated! the converted text is:\n ', kotodama_prompt)
          


        Kotodama = importable.Kotodama_Sampler(importable.model, text=speaker + kotodama_prompt, device=importable.device) 

        wav, s_prev = importable.Longform(text_input, 
                                s_prev, 
                                Kotodama, 
                                alpha = alpha, 
                                beta = beta, 
                                t = t, 
                                diffusion_steps=diffusion_steps, embedding_scale=embedding_scale, rate_of_speech=rate_of_speech)
        wavs.append(wav)
        wavs.append(np.zeros(int(silence)))
        
    print('Synthesized: ')
    return (24000, np.concatenate(wavs))
    
    

def Inference_Synth_Prompt(text, description, Kotodama, alpha, beta, diffusion_steps, embedding_scale, rate_of_speech , progress=gr.Progress()):
    
    if is_japanese(text):
      text = text

      
    else:
      text = importable.p2g(smart_phonemize(text))
      
      print('kimia activated! the converted text is: ', text)
      
    
    prompt = f"""{description} \n text: {text}"""
    
    print('prompt ->: ', prompt)

    text = smart_phonemize(text)
    
    print('phonemes ->: ', text)

    Kotodama = importable.Kotodama_Prompter(importable.model, text=prompt, device=importable.device) 

    wav = importable.inference(text, 
                            Kotodama, 
                            alpha = alpha, 
                            beta = beta, 
                            diffusion_steps=diffusion_steps, embedding_scale=embedding_scale, rate_of_speech=rate_of_speech)
    
    wav = importable.trim_long_silences(wav)


    print('Synthesized: ')
    return (24000, wav)

with gr.Blocks() as audio_inf:
    with gr.Row():
        with gr.Column(scale=1):
            inp = gr.Textbox(label="Text", info="Enter the text", value="ãã¿ã®å­˜åœ¨ã¯ã€ç§ã®å¿ƒã®ä¸­ã§ç‡ƒãˆã‚‹å°ã•ãªå…‰ã®ã‚ˆã†ã€‚ãã¿ãŒã„ãªã„æ™‚ã€ä¸–ç•Œã¯ç™½é»’ã®å†™çœŸã¿ãŸã„ã«å¯‚ã—ãã¦ã€ä½•ã‚‚è¼ã„ã¦ãªã„ã€‚ãã¿ã®ç¬‘é¡”ã ã‘ãŒã€ç§ã®ç°è‰²ã®æ—¥ã€…ã«è‰²ã‚’å¡—ã£ã¦ãã‚Œã‚‹ã€‚é›¢ã‚Œã¦ã‚‹æ™‚é–“ã¯ã€ã‚ã¡ã‚ƒãã¡ã‚ƒé•·ãæ„Ÿã˜ã‚‰ã‚Œã¦ã€ãã¿ã¸ã®æƒ³ã„ã¯é¢¨èˆ¹ã¿ãŸã„ã«ã©ã‚“ã©ã‚“è†¨ã‚‰ã‚“ã§ã„ãã€‚ãã¿ãªã—ã®ä¸–ç•Œãªã‚“ã¦ã€æƒ³åƒã§ããªã„ã‚ˆã€‚",  interactive=True, scale=5)
            voice = gr.Dropdown(voicelist, label="Voice", info="Select a default voice.", value=voicelist[5], interactive=True)
            voice_2 = gr.Audio(label="Upload your own Audio", interactive=True, type='filepath', max_length=300, waveform_options={'waveform_color': '#a3ffc3', 'waveform_progress_color': '#e972ab'})
            
            with gr.Accordion("Advanced Parameters", open=False):

                alpha = gr.Slider(minimum=0, maximum=1, value=0.0, step=0.1, label="Alpha", info="a Diffusion sampler parameter handling the timbre, higher means less affected by the reference | 0 = diffusion is disabled", interactive=True)
                beta = gr.Slider(minimum=0, maximum=1, value=0.0, step=0.1, label="Beta", info="a Diffusion sampler parameter, higher means less affected by the reference | 0 = diffusion is disabled", interactive=True)
                multispeakersteps = gr.Slider(minimum=3, maximum=15, value=5, step=1, label="Diffusion Steps", interactive=True)
                embscale = gr.Slider(minimum=1, maximum=5, value=1, step=0.1, label="Intensity", info="will impact the expressiveness, if you raise it too much it'll break.", interactive=True)
                rate_of_speech = gr.Slider(minimum=0.5, maximum=2, value=1, step=0.1, label="Rate of Speech", info="Higher -> Faster", interactive=True)
        
        with gr.Column(scale=1):
            btn = gr.Button("Synthesize", variant="primary")
            audio = gr.Audio(interactive=False, label="Synthesized Audio", waveform_options={'waveform_color': '#a3ffc3', 'waveform_progress_color': '#e972ab'})
            btn.click(Synthesize_Audio, inputs=[inp, voice, voice_2, multispeakersteps, embscale, alpha, beta, rate_of_speech], outputs=[audio], concurrency_limit=4)

# Kotodama Text sampler Synthesis Block
with gr.Blocks() as longform:
    with gr.Row():
        with gr.Column(scale=1):
            inp_longform = gr.Textbox(
                label="Text",
                info="Enter the text [Speaker: Text] | Also works without any name.",
                value=list(example_texts.values())[0],  
                interactive=True,
                scale=5
            )
            
            with gr.Row():
                example_dropdown = gr.Dropdown(
                    choices=list(example_texts.keys()),  
                    label="Example Texts [pick one!]",
                    value=list(example_texts.keys())[0], 
                    interactive=True
                )
                
            example_dropdown.change(
                fn=update_text_input,
                inputs=[example_dropdown],
                outputs=[inp_longform]
            )
            
            with gr.Accordion("Advanced Parameters", open=False):

                alpha_longform = gr.Slider(minimum=0, maximum=1, value=0.0, step=0.1, 
                                           label="Alpha", 
                                           info="a Diffusion parameter handling the timbre, higher means less affected by the reference | 0 = diffusion is disabled", 
                                           interactive=True)
                beta_longform = gr.Slider(minimum=0, maximum=1, value=0.0, step=0.1, 
                                          label="Beta", 
                                          info="a Diffusion parameter, higher means less affected by the reference | 0 = diffusion is disabled", 
                                          interactive=True)
                diffusion_steps_longform = gr.Slider(minimum=3, maximum=15, value=10, step=1, 
                                                     label="Diffusion Steps", 
                                                     interactive=True)
                embedding_scale_longform = gr.Slider(minimum=1, maximum=5, value=1.25, step=0.1, 
                                              label="Intensity", 
                                              info="a Diffusion parameter, it will impact the expressiveness, if you raise it too much it'll break.", 
                                              interactive=True)

                rate_of_speech_longform = gr.Slider(minimum=0.5, maximum=2, value=1, step=0.1, 
                                                    label="Rate of Speech", 
                                                    info="Higher = Faster", 
                                                    interactive=True)

        with gr.Column(scale=1):
            btn_longform = gr.Button("Synthesize", variant="primary")
            audio_longform = gr.Audio(interactive=False, 
                                      label="Synthesized Audio", 
                                      waveform_options={'waveform_color': '#a3ffc3', 'waveform_progress_color': '#e972ab'})
            
            btn_longform.click(LongformSynth_Text, 
                                inputs=[inp_longform, 
                                        gr.State(None),  # s_prev 
                                        gr.State(None),  # Kotodama
                                        alpha_longform, 
                                        beta_longform, 
                                        gr.State(.8),   # t parameter 
                                        diffusion_steps_longform, 
                                        embedding_scale_longform, 
                                        rate_of_speech_longform], 
                                outputs=[audio_longform], 
                                concurrency_limit=4)

# Kotodama prompt sampler Inference Block
with gr.Blocks() as prompt_inference:
    with gr.Row():
        with gr.Column(scale=1):
            text_prompt = gr.Textbox(
                label="Text", 
                info="Enter the text to synthesize. This text will also be fed to the encoder. Make sure to see the Read Me for more details!",
                value=inputs[0],
                interactive=True,
                scale=5
            )
            description_prompt = gr.Textbox(
                label="Description",
                info="Enter a highly detailed, descriptive prompt that matches the vibe of your text to guide the synthesis.",
                value=prompts[0],
                interactive=True, 
                scale=7
            )
            
            with gr.Row():
                random_btn = gr.Button('Random Example', variant='secondary')
            
            with gr.Accordion("Advanced Parameters", open=True):
                embedding_scale_prompt = gr.Slider(minimum=1, maximum=5, value=1, step=0.25,
                                            label="Intensity",
                                            info="it will impact the expressiveness, if you raise it too much it'll break.",
                                            interactive=True)
                alpha_prompt = gr.Slider(minimum=0, maximum=1, value=0.0, step=0.1,
                                         label="Alpha",
                                         info="a Diffusion sampler parameter handling the timbre, higher means less affected by the reference | 0 = diffusion is disabled",
                                         interactive=True)
                beta_prompt = gr.Slider(minimum=0, maximum=1, value=0.0, step=0.1,
                                        label="Beta",
                                        info="a Diffusion sampler parameter, higher means less affected by the reference | 0 = diffusion is disabled",
                                        interactive=True)
                diffusion_steps_prompt = gr.Slider(minimum=3, maximum=15, value=10, step=1,
                                                   label="Diffusion Steps",
                                                   interactive=True)
                rate_of_speech_prompt = gr.Slider(minimum=0.5, maximum=2, value=1, step=0.1,
                                                  label="Rate of Speech",
                                                  info="Higher = Faster",
                                                  interactive=True)
        with gr.Column(scale=1):
            btn_prompt = gr.Button("Synthesize with Prompt", variant="primary")
            audio_prompt = gr.Audio(interactive=False,
                                    label="Prompt-based Synthesized Audio",
                                    waveform_options={'waveform_color': '#a3ffc3', 'waveform_progress_color': '#e972ab'})
           
 
            random_btn.click(
                fn=get_random_prompt_pair,
                inputs=[],
                outputs=[text_prompt, description_prompt]
            )
            
            btn_prompt.click(Inference_Synth_Prompt,
                              inputs=[text_prompt,
                                      description_prompt,
                                      gr.State(None),
                                      alpha_prompt,
                                      beta_prompt,
                                      diffusion_steps_prompt,
                                      embedding_scale_prompt,
                                      rate_of_speech_prompt],
                              outputs=[audio_prompt],
                              concurrency_limit=4)

notes = """
<h1>Notes</h1>

<p>
This work is somewhat different from your typical speech model. It offers a high degree of control<br>
over the generation process, which means it's easy to inadvertently produce unimpressive outputs.
</p>

<p>
<b>Kotodama</b> and the <b>Diffusion sampler</b> can significantly help guide the generation towards<br>
something that aligns with your input, but they aren't foolproof. turn off the diffusion sampler or <br>
set it to very low values if it doesn't sound good to you. <br>
</p>

<p>
The prompt encoder is also highly experimental and should be treated as a proof of concept. Due to the<br>
overwhelming ratio of female to male speakers and the wide variation in both speakers and their expressions,<br>
the prompt encoder may occasionally produce subpar or contradicting outputs. For example, high expressiveness alongside <br>
high pitch has been associated with females speakers simply because I had orders of magnitude more of them in the dataset.<br>
</p>

<p>
________________________________________________________ <br>
<strong>A useful note about the voice design and prompting:</strong><br>\n
The vibe of the dialogue impacts the generated voice since the Japanese dialogue  <br>
and the prompts were jointly trained. This is a peculiar feature of the Japanese lanuage.<br>
For example if you use ä¿º (ore)ã€åƒ•(boku) or your input is overall masculine  <br>
you may get a guy's voice, even if you describe it as female in the prompt. <br> \n
The Japanese text that is fed to the prompt doesn't necessarily have to be  <br>
the same as your input, but we can't do it in this demo <br>
to not make the page too convoluted. In a real world scenario, you can just use a <br>
prompt with a suitable Japanese text to guide the model, get the style<br>
then move on to apply it to whatever dialogue you wish your model to speak.<br>


</p>
________________________________________________________ <br>
<p>
The pitch information in my data was accurately calculated, but it only works in comparison to the other speakers <br>
so you may find a deep pitch may not be exactly too deep; although it actually is <br> 
when you compare it to others within the same data, also some of the gender labels <br>
are inaccurate since we used a model to annotate them. <br> \n
The main goal of this inference method is to demonstrate that style can be mapped to description's embeddings <br>
yielding reasonably good results.
</p>

<p>
Overall, I'm confident that with a bit of experimentation, you can achieve reasonbaly good results. <br>
The model should work well out of the box 90% of the time without the need for extensive tweaking.<br>
However, here are some tips in case you encounter issues:
</p>

<h2>Tips:</h2>

<ul>
  <li>
    Ensure that your input closely matches your reference (audio or text prompt) in terms of tone,<br>
    non-verbal cues, duration, etc.
  </li>
  
  <li>
    If your audio is too long but the input is too short, the speech rate will be slow, and vice versa.
  </li>
  
  <li>
    Experiment with the <b>alpha</b>, <b>beta</b>, and <b>Intensity</b> parameters. The Diffusion<br>
    sampler is non-deterministic, so regenerate a few times if you're not satisfied with the output.
  </li>
  
  <li>
    The speaker's share and expressive distribution in the dataset significantly impact the quality;<br>
    you won't necessarily get perfect results with all speakers.
  </li>
  
  <li>
    Punctuation is very important, for example if you add Â«!Â» mark it will raise the voice or make it more intense.
  </li>
  
  <li>
    Not all speakers are equal. Less represented speakers or out-of-distribution inputs may result<br>
    in artifacts.
  </li>
  
  <li>
    If the Diffusion sampler works but the speaker didn't have a certain expression (e.g., extreme anger)<br>
    in the dataset, try raising the diffusion sampler's parameters and let it handle everything. Though<br>
    it may result in less speaker similarity, the ideal way to handle this is to cook new vectors by<br>
    transferring an emotion from one speaker to another. But you can't do that in this space.
  </li>
  
  <li>
    For voice-based inference, you can use litagin's awesome <a href="https://huggingface.co/datasets/litagin/Moe-speech" target="_blank">Moe-speech dataset</a>,<br>
    as part of the training data includes a portion of that.
  </li>
  
  <li>
    you may also want to tweak the phonemes if you're going for something wild. <br>
    i have used cutlet in the backend, but that doesn't seem to like some of my mappings.
  </li>


</ul>
"""


notes_jp = """
<h1>ãƒ¡ãƒ¢</h1>

<p>
ã“ã®ä½œæ¥­ã¯ã€å…¸å‹çš„ãªã‚¹ãƒ”ãƒ¼ãƒãƒ¢ãƒ‡ãƒ«ã¨ã¯å°‘ã—ç•°ãªã‚Šã¾ã™ã€‚ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã«å¯¾ã—ã¦é«˜ã„åˆ¶å¾¡ã‚’æä¾›ã™ã‚‹ãŸã‚ã€æ„å›³ã›ãšã«<br>
æ¯”è¼ƒçš„ã«ã‚¯ã‚ªãƒªãƒ†ã‚£ãƒ¼ã®ä½ã„å‡ºåŠ›ã‚’ç”Ÿæˆã—ã¦ã—ã¾ã†ã“ã¨ãŒå®¹æ˜“ã§ã™ã€‚
</p>

<p>
<b>Kotodama</b>ã¨<b>Diffusionã‚µãƒ³ãƒ—ãƒ©ãƒ¼</b>ã¯ã€å…¥åŠ›ã«æ²¿ã£ãŸã‚‚ã®ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®å¤§ããªåŠ©ã‘ã¨ãªã‚Šã¾ã™ãŒã€<br>
ä¸‡å…¨ã¨ã„ã†ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚è‰¯ã„ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆãŒå‡ºãªã„å ´åˆã¯ã€ãƒ‡ã‚£ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’ã‚ªãƒ•ã«ã™ã‚‹ã‹ã€éå¸¸ã«ä½ã„å€¤ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚
</p>


_____________________________________________<br>\n
<strong>éŸ³å£°ãƒ‡ã‚¶ã‚¤ãƒ³ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«é–¢ã™ã‚‹æœ‰ç”¨ãªãƒ¡ãƒ¢:</strong><br>
ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã®é›°å›²æ°—ã¯ã€æ—¥æœ¬èªã®ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå…±åŒã§Trainã•ã‚ŒãŸãŸã‚ã€ç”Ÿæˆã•ã‚Œã‚‹éŸ³å£°ã«å½±éŸ¿ã‚’ä¸ãˆã¾ã™ã€‚<br>
ã“ã‚Œã¯æ—¥æœ¬èªã®ç‰¹å¾´çš„ãªæ©Ÿèƒ½ã§ã™ã€‚ä¾‹ãˆã°ã€ã€Œä¿ºã€ã‚„ã€Œåƒ•ã€ã‚’ä½¿ç”¨ã—ãŸã‚Šã€å…¨ä½“çš„ã«ç”·æ€§ã‚‰ã—ã„å…¥åŠ›ã‚’ã™ã‚‹ã¨ã€<br>
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å¥³æ€§ã¨è¨˜è¿°ã—ã¦ã„ã¦ã‚‚ã€ç”·æ€§ã®å£°ãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚<br>
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å…¥åŠ›ã•ã‚Œã‚‹æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã¯ã€å¿…ãšã—ã‚‚å…¥åŠ›å†…å®¹ã¨åŒã˜ã§ã‚ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€<br>
ã“ã®ãƒ‡ãƒ¢ã§ã¯ãƒšãƒ¼ã‚¸ãŒè¤‡é›‘ã«ãªã‚Šã™ããªã„ã‚ˆã†ã«ãã‚Œã‚’è¡Œã†ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚<br>
å®Ÿéš›ã®ã‚·ãƒŠãƒªã‚ªã§ã¯ã€é©åˆ‡ãªæ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å°ãã€<br>
ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å–å¾—ã—ãŸå¾Œã€ãã‚Œã‚’å¸Œæœ›ã™ã‚‹ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã«é©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚<br>

_____________________________________________<br>\n

<p>
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚‚éå¸¸ã«å®Ÿé¨“çš„ã§ã‚ã‚Šã€æ¦‚å¿µå®Ÿè¨¼ã¨ã—ã¦æ‰±ã†ã¹ãã§ã™ã€‚å¥³æ€§è©±è€…å¯¾ç”·æ€§è©±è€…ã®æ¯”ç‡ãŒåœ§å€’çš„ã§ã€<br>
ã¾ãŸè©±è€…ã¨ãã®è¡¨ç¾ã«å¤§ããªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹ãŸã‚ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯è³ªã®ä½ã„å‡ºåŠ›ã‚’ç”Ÿæˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚<br>
ä¾‹ãˆã°ã€é«˜ã„è¡¨ç¾åŠ›ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤šãå«ã¾ã‚Œã¦ã„ãŸå¥³æ€§è©±è€…ã¨é–¢é€£ä»˜ã‘ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚<br>
ãã‚Œã«ã€ãƒ‡ãƒ¼ã‚¿ã®ãƒ”ãƒƒãƒæƒ…å ±ã¯æ­£ç¢ºã«è¨ˆç®—ã•ã‚Œã¾ã—ãŸãŒã€ãã‚Œã¯ä»–ã®ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã¨ã®æ¯”è¼ƒã§ã—ã‹æ©Ÿèƒ½ã—ã¾ã›ã‚“...<br>
ã ã‹ã‚‰ã€æ·±ã„ãƒ”ãƒƒãƒãŒå¿…ãšã—ã‚‚æ·±ã™ãã‚‹ã‚ã‘ã§ã¯ãªã„ã“ã¨ã«æ°—ã¥ãã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚<br>
ãŸã ã—ã€å®Ÿéš›ã«ã¯ã€åŒã˜ãƒ‡ãƒ¼ã‚¿å†…ã®ä»–ã®äººã¨æ¯”è¼ƒã™ã‚‹ã¨ã€æ·±ã™ãã¾ã™ã€‚ã“ã®ã‚¤ãƒ³ãƒ•ãƒ¬ãƒ³ã‚¹ã®ä¸»ãªç›®çš„ã¯ã€<br>
ã‚¹ã‚¿ã‚¤ãƒ«ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨˜è¿°ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã€åˆç†çš„ã«è‰¯ã„çµæœã‚’å¾—ã‚‹ã“ã¨ã«ã‚ã‚Šã¾ã™ã€‚
</p>

<p>
å…¨ä½“ã¨ã—ã¦ã€å°‘ã—ã®å®Ÿé¨“ã§ã»ã¼æœ›ã‚€çµæœã‚’é”æˆã§ãã‚‹ã¨è‡ªä¿¡ã‚’æŒã£ã¦ã„ã¾ã™ã€‚90%ã®ã‚±ãƒ¼ã‚¹ã§ã€å¤§å¹…ãªèª¿æ•´ã‚’å¿…è¦ã¨ã›ãšã€<br>
ãã®ã¾ã¾ã§ã†ã¾ãå‹•ä½œã™ã‚‹ã¯ãšã§ã™ã€‚ã—ã‹ã—ã€å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã®ãŸã‚ã«ã„ãã¤ã‹ã®ãƒ’ãƒ³ãƒˆãŒã‚ã‚Šã¾ã™ï¼š
</p>

<h2>ãƒ’ãƒ³ãƒˆï¼š</h2>

<ul>
  <li>
    å…¥åŠ›ãŒãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ï¼ˆéŸ³å£°ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã¨ãƒˆãƒ¼ãƒ³ã€éè¨€èªçš„ãªæ‰‹ãŒã‹ã‚Šã€<br>
    é•·ã•ãªã©ã§å¯†æ¥ã«ä¸€è‡´ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
  </li>
  
  <li>
    éŸ³å£°ãŒé•·ã™ãã‚‹ãŒå…¥åŠ›ãŒçŸ­ã™ãã‚‹å ´åˆã€è©±é€ŸãŒé…ããªã‚Šã¾ã™ã€‚ãã®é€†ã‚‚ã¾ãŸåŒæ§˜ã§ã™ã€‚
  </li>
  
  <li>
    ã‚¢ãƒ«ãƒ•ã‚¡ã€ãƒ™ãƒ¼ã‚¿ã€ãŠã‚ˆã³åŸ‹ã‚è¾¼ã¿ã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è©¦è¡ŒéŒ¯èª¤ã—ã¦ãã ã•ã„ã€‚Diffusionã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã¯<br>
    éæ±ºå®šçš„ãªã®ã§ã€æº€è¶³ã®ã„ãå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œãªã„å ´åˆã¯ä½•åº¦ã‹å†ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
  </li>
  
  <li>
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®è©±è€…ã®åˆ†å¸ƒã¨è¡¨ç¾åŠ›ã®åˆ†å¸ƒã¯å“è³ªã«å¤§ããå½±éŸ¿ã—ã¾ã™ã€‚<br>
    ã™ã¹ã¦ã®è©±è€…ã§å¿…ãšã—ã‚‚å®Œç’§ãªçµæœãŒå¾—ã‚‰ã‚Œã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
  </li>
  
  <li>
    å¥èª­ç‚¹ã¯é‡è¦ã§ã™ã€‚ãŸã¨ãˆãªã€ã€Œï¼ã€ã‚’ä½¿ãˆã°ã€ã‚¹ã‚¿ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ†ãƒ³ã‚·ãƒ†ã‚£ãŒä¸ŠãŒã‚Šã¾ã™ã€‚
  </li>
  
  <li>
    ã™ã¹ã¦ã®è©±è€…ãŒå¹³ç­‰ã«è¡¨ç¾ã•ã‚Œã¦ã„ã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å°‘ãªã„è¡¨ç¾ã®è©±è€…ã‚„<br>
    åˆ†å¸ƒå¤–ã®å…¥åŠ›ã¯ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’ç”Ÿã˜ã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
  </li>
  
  <li>
    Diffusionã‚µãƒ³ãƒ—ãƒ©ãƒ¼ãŒæ©Ÿèƒ½ã—ã¦ã„ã‚‹ãŒã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã§ç‰¹å®šã®è¡¨ç¾ï¼ˆä¾‹ï¼šæ¥µåº¦ã®æ€’ã‚Šï¼‰ãŒãªã„å ´åˆã€<br>
    Diffusionã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¼•ãä¸Šã’ã€ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã«ã™ã¹ã¦ã‚’ä»»ã›ã¦ãã ã•ã„ã€‚ãŸã ã—ã€ãã‚Œã«ã‚ˆã‚Š<br>
    è©±è€…ã®é¡ä¼¼æ€§ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®å•é¡Œã‚’ç†æƒ³çš„ã«è§£æ±ºã™ã‚‹æ–¹æ³•ã¯ã€ã‚ã‚‹è©±è€…ã‹ã‚‰åˆ¥ã®è©±è€…ã«<br>
    æ„Ÿæƒ…ã‚’è»¢é€ã—æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã™ãŒã€ã“ã“ã§ã¯ã§ãã¾ã›ã‚“ã€‚
  </li>
  
  <li>
    éŸ³å£°ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ãƒ•ãƒ¬ãƒ³ã‚¹ã«ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã¨ã—ã¦Moe-speechãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸€éƒ¨ã‚’å«ã‚€<br>
    <a href="https://huggingface.co/datasets/litagin/Moe-speech" target="_blank">litaginã®ç´ æ™´ã‚‰ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</a>ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚
  </li>
  
  <li>
    ãŸã¾ã«ã¯éŸ³ç´ ã®èª¿æ•´ãŒå¿…è¦ã«ãªã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™ã€‚ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ã¯cutletã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§ã™ãŒã€<br>
    ã„ãã¤ã‹ã®OODãƒãƒƒãƒ”ãƒ³ã‚°ãŒcutletã¨ç›¸æ€§ãŒè‰¯ããªã„ã¿ãŸã„ã§ã™ã€‚
  </li>
</ul>

"""
with gr.Blocks() as read_me:
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown(notes)
    
with gr.Blocks() as read_me_jp:
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown(notes_jp)
    

custom_css = """
.tab-label {
    color: #FFD700 !important;
}
"""




with gr.Blocks(title="Tsukasa å¸", css=custom_css + "footer{display:none !important}", theme="Respair/Shiki@1.2.1") as demo:
    # gr.DuplicateButton("Duplicate Space")
    gr.Markdown(INTROTXT)


    gr.TabbedInterface([longform, audio_inf, prompt_inference, read_me, read_me_jp], 
                       ['Kotodama Text Inference', 'Voice-guided Inference','Prompt-guided Inference [Highly Experimental - not optimized]', 'Read Me! [English]', 'Read Me! [æ—¥æœ¬èª]'])

if __name__ == "__main__":
    demo.queue(api_open=False, max_size=15).launch(show_api=False, share=True)
